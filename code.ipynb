{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final CS_772_Assignment_Phase_3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jN5i-YL1dNqz",
        "nuyriDkqT_Sz",
        "fEC0XB9cUQfA",
        "1vS9WexsUVPZ",
        "NFdxrcfdUldJ",
        "5acwKNQush5r",
        "Kk27jNkTsdjO",
        "okIK0SwVtYYR",
        "c8vcLmW9rnvA",
        "5aEK7Ws9reKL",
        "WYAsfvY5reKR",
        "7T6uCRfRreKd",
        "FAAQ8UiJreKi",
        "MaNBH_r4UcRE",
        "cXdVILD7b9gr",
        "kNFlkgEPcE7z",
        "jmCPQLwncOqL",
        "Cj7uViOAP8nZ",
        "uiB2cRSVQlZg",
        "DyY0ZJ_U9XV5",
        "RmxT71tvjYyw",
        "ffNSrm9rt2kA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN5i-YL1dNqz"
      },
      "source": [
        "#### Install python packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-6Y9V3GOPQq",
        "outputId": "9c0a42f8-940b-4328-dfcc-290d04ea15be"
      },
      "source": [
        "!pip install PyDrive\n",
        "!pip install --upgrade keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.7.2)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.28.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (20.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.12.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (54.2.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n",
            "Requirement already up-to-date: keras in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuyriDkqT_Sz"
      },
      "source": [
        "#### To access pre-trained embeddings and dataset (train and test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f068QkbOOpUC"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaTLfxpojIRU"
      },
      "source": [
        "\n",
        "download = drive.CreateFile({'id':'1gVExcJs31_mHWFGNvH3YpXBnS-A_XurA'})   # replace the id with id of file you want to access\n",
        "download.GetContentFile('train.csv')        # replace the file name with your file\n",
        "\n",
        "download = drive.CreateFile({'id':'13Nc1ZZaJD7_kBup22ScZYXmxAYNg1TOT'})   # replace the id with id of file you want to access\n",
        "download.GetContentFile('test.csv')        # replace the file name with your file\n",
        "\n",
        "download = drive.CreateFile({'id':'1xyw2FyO1RTOAK-h7V3PYF_Sxmn8ho5jh'})   # replace the id with id of file you want to access\n",
        "download.GetContentFile('gold_test.csv')        # replace the file name with your file\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqlJ5Ym0jFlV"
      },
      "source": [
        "\n",
        "downloaded = drive.CreateFile({'id':'1GH7dGh9ftQikz8KIDedxBooBzX_qw0zW'})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('glove.6B.100d.txt')        # replace the file name with your file\n",
        "\n",
        "download = drive.CreateFile({'id':'1daXpP2rI4YDUHwl4I-kDolS9JfoTY_r4'})   # replace the id with id of file you want to access\n",
        "download.GetContentFile('glove.6B.200d.txt')        # replace the file name with your file\n",
        "\n",
        "download = drive.CreateFile({'id':'15IB-nj1e-E6B9PRpJslkB1RMBEtiCA6l'})   # replace the id with id of file you want to access\n",
        "download.GetContentFile('glove.6B.300d.txt')        # replace the file name with your file\n",
        "\n",
        "download = drive.CreateFile({'id':'1rH_EvmtKprvULcKj5ARNI3r3p6K9EGwO'})   # replace the id with id of file you want to access\n",
        "download.GetContentFile('wiki-news-300d-1M.vec')        # replace the file name with your file\n",
        "\n",
        "download = drive.CreateFile({'id':'1mPXuUDiPAPid2nGjnhUQIzn8Qz-nHp6V'})   # replace the id with id of file you want to access\n",
        "download.GetContentFile('GoogleNews-vectors-negative300.bin')        # replace the file name with your file"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEC0XB9cUQfA"
      },
      "source": [
        "#### Header files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5NmV1ciOT0h",
        "outputId": "3438bc68-4136-4ba6-9656-3bdd331f573c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, SimpleRNN, LSTM, Bidirectional, GRU\n",
        "from sklearn import datasets, model_selection, metrics\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.initializers import Constant\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split \n",
        "nltk.download('stopwords')\n",
        "stopword = stopwords.words('english') \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vS9WexsUVPZ"
      },
      "source": [
        "#### Preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsu00ZJwPCb5"
      },
      "source": [
        "def encode_data(tokenizer, text, tokens, preprocessing_training_data = False):\n",
        "    # This function will be used to encode the reviews using a dictionary (created using corpus vocabulary) \n",
        "\n",
        "    # Example of encoding :\"The food was fabulous but pricey\" has a vocabulary of 4 words, each one has to be mapped to an integer like: \n",
        "    # {'The':1,'food':2,'was':3 'fabulous':4 'but':5 'pricey':6} this vocabulary has to be created for the entire corpus and then be used to \n",
        "    # encode the words into integers \n",
        "\n",
        "    # return encoded examples\n",
        "    if preprocessing_training_data:\n",
        "        tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "        tokenizer.fit_on_texts(tokens)\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    return sequences, tokenizer\n",
        "\n",
        "def convert_to_lower(text):\n",
        "    # return the reviews after convering then to lowercase\n",
        "    lower_text = text.lower()\n",
        "    return lower_text\n",
        "\n",
        "def perform_tokenization(text):\n",
        "    # return the reviews after performing tokenization\n",
        "    token=nltk.word_tokenize(text)\n",
        "    return token\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    # return the reviews after removing the stopwords\n",
        "    stopword = [] # not any stopword\n",
        "    removing_stopwords=[word for word in text if word not in stopword]\n",
        "    return removing_stopwords\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # return the reviews after removing punctuations\n",
        "    removing_punctuation = [word for word in text if word.isalpha()]\n",
        "    return removing_punctuation\n",
        "\n",
        "def perform_padding(data, maxlen):\n",
        "    # return the reviews after padding the reviews to maximum length\n",
        "    padded_data = pad_sequences(data, maxlen=maxlen, padding='post')\n",
        "    return padded_data\n",
        "\n",
        "def preprocess_data(tokenizer, data, preprocessing_training_data=False, maxlen=None):\n",
        "    # make all the following function calls on your data\n",
        "    # EXAMPLE:->\n",
        "    '''\n",
        "    review = data[\"reviews\"]\n",
        "    review = convert_to_lower(review)\n",
        "    review = remove_punctuation(review)\n",
        "    review = remove_stopwords(review)\n",
        "    review = perform_tokenization(review)\n",
        "    review = encode_data(review)\n",
        "    review = perform_padding(review)\n",
        "    '''\n",
        "    # return processed data\n",
        "\n",
        "    reviews = data[\"reviews\"]\n",
        "    list_of_reviews = list(reviews)\n",
        "    string_of_reviews = ' '.join(str(e) for e in list_of_reviews)\n",
        "\n",
        "    lower_text = convert_to_lower(string_of_reviews)\n",
        "    tokens = perform_tokenization(lower_text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = remove_punctuation(tokens)\n",
        "    encoded_data, tokenizer = encode_data(tokenizer, reviews, tokens, preprocessing_training_data)\n",
        "    reviews = perform_padding(encoded_data, maxlen)\n",
        "\n",
        "    return pd.DataFrame(reviews), tokenizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFdxrcfdUldJ"
      },
      "source": [
        "### Define Models (with pre-trained embedding layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acwKNQush5r"
      },
      "source": [
        "#### Feed Forward Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTu0_OclPIol"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "\n",
        "class NeuralNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings,e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(e)\n",
        "        self.model.add(Flatten())\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk27jNkTsdjO"
      },
      "source": [
        "#### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARyYGVmWsYaw"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class LSTMNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(self.e)\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(LSTM(128))\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDxdrd4F43Bp"
      },
      "source": [
        "#### Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Ak0BZ-48o_"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class BiLSTMNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(self.e)\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(Bidirectional(LSTM(128)))\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yleil4JcOJRj"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPzquSwmOQbO"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class GRUmodel:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(self.e)\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(GRU(128))\n",
        "        #self.model.add(Dense(5 , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32 , activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYTDAdrV6uoY"
      },
      "source": [
        "#### Bi-GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWVPfh296xgr"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class BiGRUmodel:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(self.e)\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(Bidirectional(GRU(128)))\n",
        "        #self.model.add(Dense(5 , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32 , activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okIK0SwVtYYR"
      },
      "source": [
        "#### RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMi4v9AotYYW"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class RNNNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(self.e)\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(SimpleRNN(128))\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l7cqjFCNOS4"
      },
      "source": [
        "### Define Models (without pre-trained embedding layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpMsHR0CNOTA"
      },
      "source": [
        "#### Feed Forward Neural Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bmmvPF0NOTL"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "\n",
        "class NeuralNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings,e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(Embedding(vocab_size, 300, input_length=maxlen, trainable=True))\n",
        "        self.model.add(Flatten())\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjp1MqKWNOTR"
      },
      "source": [
        "#### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAznv8GvNOTW"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class LSTMNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(Embedding(vocab_size, 300, input_length=maxlen, trainable=True))\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(LSTM(128))\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXWUMEFSNOTc"
      },
      "source": [
        "#### Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zhYpikrNOTg"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class BiLSTMNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(Embedding(vocab_size, 300, input_length=maxlen, trainable=True))\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(Bidirectional(LSTM(128)))\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9NAJ2Z8NOTl"
      },
      "source": [
        "#### GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACx284g_NOTp"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class GRUmodel:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(Embedding(vocab_size, 300, input_length=maxlen, trainable=True))\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(GRU(128))\n",
        "        #self.model.add(Dense(5 , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32 , activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NN6h8o3NOTw"
      },
      "source": [
        "#### Bi-GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2rpK2PHNOT0"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class BiGRUmodel:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(Embedding(vocab_size, 300, input_length=maxlen, trainable=True))\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(Bidirectional(GRU(128)))\n",
        "        #self.model.add(Dense(5 , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32 , activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhrQFy0RNOT4"
      },
      "source": [
        "#### RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWyB2el1NOT6"
      },
      "source": [
        "def softmax_activation(x):\n",
        "    # write your own implementation from scratch and return softmax values(using predefined softmax is prohibited)\n",
        "    Exponent_calculation=tf.exp(x-tf.reduce_max(x,axis=-1,keepdims=True))\n",
        "    Normalization=tf.reduce_sum(Exponent_calculation,axis=-1,keepdims=True)\n",
        "    return Exponent_calculation/Normalization\n",
        "    \n",
        "    \n",
        "class RNNNet:\n",
        "\n",
        "    def __init__(self, reviews, ratings, e):\n",
        "        self.reviews = reviews\n",
        "        self.ratings = ratings\n",
        "        self.e = e\n",
        "    def build_nn(self):\n",
        "        #add the input and output layer here; you can use either tensorflow or pytorch\n",
        "        self.model = Sequential()\n",
        "        print(\"Reviews Shape: \", self.reviews.shape)\n",
        "        print(\"Ratings Shape: \", self.ratings.shape)\n",
        "        self.model.add(Embedding(vocab_size, 300, input_length=maxlen, trainable=True))\n",
        "        # self.model.add(Flatten())\n",
        "        self.model.add(SimpleRNN(128))\n",
        "        #self.model.add(Dense(5  , activation=\"relu\" ))\n",
        "        #self.model.add(Dense(32, activation='sigmoid'))\n",
        "        self.model.add(Dense(5))\n",
        "        self.model.add(Activation(softmax_activation, name='Softmax'))\n",
        "\n",
        "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "       \n",
        "    def train_nn(self,batch_size,epochs, ):\n",
        "        # write the training loop here; you can use either tensorflow or pytorch\n",
        "        # print validation accuracy\n",
        "        self.model.fit(self.reviews, self.ratings, epochs=epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "    def predict(self, reviews):\n",
        "        # return a list containing all the ratings predicted by the trained model\n",
        "        predicted = self.model.predict(reviews)\n",
        "        return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vcLmW9rnvA"
      },
      "source": [
        "### Import Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZuFTmtQrjJi",
        "outputId": "1b411011-a5bb-4377-d193-3a52991d607a"
      },
      "source": [
        "# reading dataset\n",
        "train_data = pd.read_csv(\"train.csv\", index_col=0)\n",
        "test_data = pd.read_csv(\"test.csv\", index_col=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "train_reviews = train_data.iloc[:, :-1]\n",
        "train_ratings = train_data.iloc[:, -1]\n",
        "train_ratings = pd.get_dummies(train_ratings)\n",
        "   \n",
        "tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "\n",
        "# pre-process data\n",
        "train_reviews, tokenizer = preprocess_data(tokenizer, train_data, preprocessing_training_data=True)\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_data, maxlen=train_reviews.shape[1])\n",
        "print(len(train_reviews))\n",
        "print(len(train_ratings))\n",
        "maxlen=train_reviews.shape[1]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "50000\n",
            "15956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aEK7Ws9reKL"
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYAsfvY5reKR"
      },
      "source": [
        "#### Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "1mafegSfreKU",
        "outputId": "9f1c7f0e-cc8e-49b7-aa5b-5613eac1f0e8"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print(train_reviews.shape)\n",
        "print(embedding_matrix.shape)\n",
        "e = Embedding(vocab_size, 300, embeddings_initializer=Constant(embedding_matrix), input_length=maxlen, trainable=False)\n",
        "# TODO: Cross-validate\n",
        "print(train_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1b57004cc77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the whole embedding into memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.300d.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.300d.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T6uCRfRreKd"
      },
      "source": [
        "#### Fasttext\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn8fcPnNreKf",
        "outputId": "6c607850-18d1-43fa-e606-e3071208e950"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('wiki-news-300d-1M.vec')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "#print(values)\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "     embedding_vector = embeddings_index.get(word)\n",
        "     if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print(train_reviews.shape)\n",
        "print(embedding_matrix.shape)\n",
        "e = Embedding(vocab_size, 300, embeddings_initializer=Constant(embedding_matrix), input_length=maxlen, trainable=True)\n",
        "# TODO: Cross-validate\n",
        "print(train_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 999995 word vectors.\n",
            "(50000, 31)\n",
            "(15956, 300)\n",
            "(50000, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAAQ8UiJreKi"
      },
      "source": [
        "####Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b2CrcKgreKl",
        "outputId": "4949de94-ff09-4b99-c109-54c21d22460e"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "\n",
        "\n",
        "#f = open('GoogleNews-vectors-negative300.bin')\n",
        "model_w2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print(type(model_w2v))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJHEK-LHNyv7"
      },
      "source": [
        "# prepare embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in model_w2v.vocab:\n",
        "        embedding_vector = model_w2v[word]\n",
        "        embedding_vector = np.array(embedding_vector)\n",
        "        if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "e = Embedding(vocab_size, 300, embeddings_initializer=Constant(embedding_matrix), input_length=maxlen, trainable=False)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bHgB-OKN3MG"
      },
      "source": [
        "### Without pre-trained word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiCX4iPzN2UK"
      },
      "source": [
        "e = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaNBH_r4UcRE"
      },
      "source": [
        "### Data Imbalanced Handling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXdVILD7b9gr"
      },
      "source": [
        "#### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgQH8sYKPPzr"
      },
      "source": [
        "def class_counts(ratings):    \n",
        "    # find # of elements of each class\n",
        "    cnts = ratings.value_counts()\n",
        "    class1_count = cnts[1]\n",
        "    class2_count = cnts[2]\n",
        "    class3_count = cnts[3]\n",
        "    class4_count = cnts[4]\n",
        "    class5_count = cnts[5]\n",
        "    \n",
        "    return class1_count, class2_count, class3_count, class4_count, class5_count\n",
        "\n",
        "def minority_class_count(ratings):\n",
        "    # find # no of elements in the minority class\n",
        "    return min(class_counts(ratings))\n",
        "\n",
        "def majority_class_count(ratings):\n",
        "    # find # no of elements in the minority class\n",
        "    return max(class_counts(ratings))\n",
        "\n",
        "def average_class_count(ratings):\n",
        "    return int(sum(class_counts(ratings))/5) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNFlkgEPcE7z"
      },
      "source": [
        "####  Undersampling Technique-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6brLxAQbwwh",
        "outputId": "5c104567-00fe-4fa9-91e6-2b04628850c3"
      },
      "source": [
        "# 1. reduce the number of ratings of each class to the 'minority_class_count' \n",
        "class1_data = train_data[train_data['ratings']==1]\n",
        "class2_data = train_data[train_data['ratings']==2]\n",
        "class3_data = train_data[train_data['ratings']==3]\n",
        "class4_data = train_data[train_data['ratings']==4]\n",
        "class5_data = train_data[train_data['ratings']==5]\n",
        "\n",
        "minority_class_cnt = minority_class_count(train_data['ratings'])\n",
        "class1_data = class1_data.sample(minority_class_cnt, random_state=1)\n",
        "class2_data = class2_data.sample(minority_class_cnt, random_state=1)\n",
        "class3_data = class3_data.sample(minority_class_cnt, random_state=1)\n",
        "class4_data = class4_data.sample(minority_class_cnt, random_state=1)\n",
        "class5_data = class5_data.sample(minority_class_cnt, random_state=1)\n",
        "\n",
        "train_data_undersample = pd.concat([class1_data, class2_data, class3_data, class4_data, class5_data], axis=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "train_reviews = train_data_undersample.iloc[:, :-1]\n",
        "train_ratings = train_data_undersample.iloc[:, -1]\n",
        "train_ratings = pd.get_dummies(train_ratings)\n",
        "   \n",
        "tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "\n",
        "# pre-process data\n",
        "train_reviews, tokenizer = preprocess_data(tokenizer, train_data_undersample, preprocessing_training_data=True)\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_data, maxlen=train_reviews.shape[1])\n",
        "\n",
        "maxlen=train_reviews.shape[1]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "print(\"Train Data Shape:\", train_reviews.shape, \"Test Data Shape:\", test_reviews.shape)\n",
        "print(f\"Train Data Distribution:\\n{train_data_undersample['ratings'].value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 8243\n",
            "Train Data Shape: (11325, 30) Test Data Shape: (10000, 30)\n",
            "Train Data Distribution:\n",
            "5    2265\n",
            "4    2265\n",
            "3    2265\n",
            "2    2265\n",
            "1    2265\n",
            "Name: ratings, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmCPQLwncOqL"
      },
      "source": [
        "#### Undersampling technique-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMon7DHQbwnN",
        "outputId": "cd57924c-cc25-4db2-c365-b67f5631a4a0"
      },
      "source": [
        "# 2. reduce the number of ratings of each class to the 'avg_class_count' \n",
        "class1_data = train_data[train_data['ratings']==1]\n",
        "class2_data = train_data[train_data['ratings']==2]\n",
        "class3_data = train_data[train_data['ratings']==3]\n",
        "class4_data = train_data[train_data['ratings']==4]\n",
        "class5_data = train_data[train_data['ratings']==5]\n",
        "\n",
        "avg_class_count = average_class_count(train_data['ratings'])\n",
        "class1_data = class1_data.sample(min(class1_data.shape[0], avg_class_count), random_state=1)\n",
        "class2_data = class2_data.sample(min(class2_data.shape[0], avg_class_count), random_state=1)\n",
        "class3_data = class3_data.sample(min(class3_data.shape[0], avg_class_count), random_state=1)\n",
        "class4_data = class4_data.sample(min(class4_data.shape[0], avg_class_count), random_state=1)\n",
        "class5_data = class5_data.sample(min(class5_data.shape[0], avg_class_count), random_state=1)\n",
        "\n",
        "train_data_undersample = pd.concat([class1_data, class2_data, class3_data, class4_data, class5_data], axis=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "train_reviews = train_data_undersample.iloc[:, :-1]\n",
        "train_ratings = train_data_undersample.iloc[:, -1]\n",
        "train_ratings = pd.get_dummies(train_ratings)\n",
        "   \n",
        "tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "\n",
        "# pre-process data\n",
        "train_reviews, tokenizer = preprocess_data(tokenizer, train_data_undersample, preprocessing_training_data=True)\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_data, maxlen=train_reviews.shape[1])\n",
        "\n",
        "maxlen=train_reviews.shape[1]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "print(\"Train Data Shape:\", train_reviews.shape, \"Test Data Shape:\", test_reviews.shape)\n",
        "print(f\"Train Data Distribution:\\n{train_data_undersample['ratings'].value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 12294\n",
            "Train Data Shape: (26807, 31) Test Data Shape: (10000, 31)\n",
            "Train Data Distribution:\n",
            "5    10000\n",
            "4     6871\n",
            "1     4059\n",
            "3     3612\n",
            "2     2265\n",
            "Name: ratings, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3l5LVS8cY1Q"
      },
      "source": [
        "#### Oversampling Technique-1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjv5TTLqccHH",
        "outputId": "39d9c6ad-b6ae-48c8-8a8a-bb9a9f2ac35e"
      },
      "source": [
        "# 1. increase the number of ratings of each class to the 'majority_class_count' \n",
        "class1_data = train_data[train_data['ratings']==1]\n",
        "class2_data = train_data[train_data['ratings']==2]\n",
        "class3_data = train_data[train_data['ratings']==3]\n",
        "class4_data = train_data[train_data['ratings']==4]\n",
        "class5_data = train_data[train_data['ratings']==5]\n",
        "\n",
        "majority_class_cnt = majority_class_count(train_data['ratings'])\n",
        "class1_data = class1_data.sample(majority_class_cnt, random_state=1, replace=True)\n",
        "class2_data = class2_data.sample(majority_class_cnt, random_state=1, replace=True)\n",
        "class3_data = class3_data.sample(majority_class_cnt, random_state=1, replace=True)\n",
        "class4_data = class4_data.sample(majority_class_cnt, random_state=1, replace=True)\n",
        "class5_data = class5_data.sample(majority_class_cnt, random_state=1, replace=True)\n",
        "\n",
        "train_data_oversample = pd.concat([class1_data, class2_data, class3_data, class4_data, class5_data], axis=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "train_reviews = train_data_undersample.iloc[:, :-1]\n",
        "train_ratings = train_data_undersample.iloc[:, -1]\n",
        "train_ratings = pd.get_dummies(train_ratings)\n",
        "   \n",
        "tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "\n",
        "# pre-process data\n",
        "train_reviews, tokenizer = preprocess_data(tokenizer, train_data_oversample, preprocessing_training_data=True)\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_data, maxlen=train_reviews.shape[1])\n",
        "\n",
        "maxlen=train_reviews.shape[1]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "print(\"Train Data Shape:\", train_reviews.shape, \"Test Data Shape:\", test_reviews.shape)\n",
        "print(f\"Train Data Distribution:\\n{train_data_oversample['ratings'].value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 14210\n",
            "Train Data Shape: (165965, 31) Test Data Shape: (10000, 31)\n",
            "Train Data Distribution:\n",
            "5    33193\n",
            "4    33193\n",
            "3    33193\n",
            "2    33193\n",
            "1    33193\n",
            "Name: ratings, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSr7yoTscium"
      },
      "source": [
        "#### Oversampling Technique-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVuXnTI3cllj",
        "outputId": "441187e7-bc93-4cea-81fd-87431d42f813"
      },
      "source": [
        "# 2. increase the number of ratings of each class to the 'avg_class_count' \n",
        "class1_data = train_data[train_data['ratings']==1]\n",
        "class2_data = train_data[train_data['ratings']==2]\n",
        "class3_data = train_data[train_data['ratings']==3]\n",
        "class4_data = train_data[train_data['ratings']==4]\n",
        "class5_data = train_data[train_data['ratings']==5]\n",
        "\n",
        "avg_class_count = average_class_count(train_data['ratings'])\n",
        "class1_data = class1_data.sample(max(class1_data.shape[0], avg_class_count), random_state=1, replace=True)\n",
        "class2_data = class2_data.sample(max(class2_data.shape[0], avg_class_count), random_state=1, replace=True)\n",
        "class3_data = class3_data.sample(max(class3_data.shape[0], avg_class_count), random_state=1, replace=True)\n",
        "class4_data = class4_data.sample(max(class4_data.shape[0], avg_class_count), random_state=1, replace=True)\n",
        "class5_data = class5_data.sample(max(class5_data.shape[0], avg_class_count), random_state=1, replace=True)\n",
        "\n",
        "train_data_oversample = pd.concat([class1_data, class2_data, class3_data, class4_data, class5_data], axis=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "train_reviews = train_data_undersample.iloc[:, :-1]\n",
        "train_ratings = train_data_undersample.iloc[:, -1]\n",
        "train_ratings = pd.get_dummies(train_ratings)\n",
        "   \n",
        "tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "\n",
        "# pre-process data\n",
        "train_reviews, tokenizer = preprocess_data(tokenizer, train_data_oversample, preprocessing_training_data=True)\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_data, maxlen=train_reviews.shape[1])\n",
        "\n",
        "maxlen=train_reviews.shape[1]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "print(\"Train Data Shape:\", train_reviews.shape, \"Test Data Shape:\", test_reviews.shape)\n",
        "print(f\"Train Data Distribution:\\n{train_data_oversample['ratings'].value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 13847\n",
            "Train Data Shape: (73193, 31) Test Data Shape: (10000, 31)\n",
            "Train Data Distribution:\n",
            "5    33193\n",
            "4    10000\n",
            "3    10000\n",
            "2    10000\n",
            "1    10000\n",
            "Name: ratings, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXAOF039clD8"
      },
      "source": [
        "#### Oversampling & Undersampling Technique-3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMDNn5Sjcx3K",
        "outputId": "2fd7b104-dd5e-42c2-bd44-82eb23389f73"
      },
      "source": [
        "# 3. change the number of ratings of each class to the 'avg_class_count' \n",
        "class1_data = train_data[train_data['ratings']==1]\n",
        "class2_data = train_data[train_data['ratings']==2]\n",
        "class3_data = train_data[train_data['ratings']==3]\n",
        "class4_data = train_data[train_data['ratings']==4]\n",
        "class5_data = train_data[train_data['ratings']==5]\n",
        "\n",
        "avg_class_count = average_class_count(train_data['ratings'])\n",
        "class1_data = class1_data.sample(avg_class_count, random_state=1, replace=True)\n",
        "class2_data = class2_data.sample(avg_class_count, random_state=1, replace=True)\n",
        "class3_data = class3_data.sample(avg_class_count, random_state=1, replace=True)\n",
        "class4_data = class4_data.sample(avg_class_count, random_state=1, replace=True)\n",
        "class5_data = class5_data.sample(avg_class_count, random_state=1, replace=True)\n",
        "\n",
        "train_data_oversample = pd.concat([class1_data, class2_data, class3_data, class4_data, class5_data], axis=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "train_reviews = train_data_undersample.iloc[:, :-1]\n",
        "train_ratings = train_data_undersample.iloc[:, -1]\n",
        "train_ratings = pd.get_dummies(train_ratings)\n",
        "   \n",
        "tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "\n",
        "# pre-process data\n",
        "train_reviews, tokenizer = preprocess_data(tokenizer, train_data_oversample, preprocessing_training_data=True)\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_data, maxlen=train_reviews.shape[1])\n",
        "\n",
        "maxlen=train_reviews.shape[1]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Vocab Size:\", vocab_size)\n",
        "print(\"Train Data Shape:\", train_reviews.shape, \"Test Data Shape:\", test_reviews.shape)\n",
        "print(f\"Train Data Distribution:\\n{train_data_oversample['ratings'].value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size: 11468\n",
            "Train Data Shape: (50000, 31) Test Data Shape: (10000, 31)\n",
            "Train Data Distribution:\n",
            "5    10000\n",
            "4    10000\n",
            "3    10000\n",
            "2    10000\n",
            "1    10000\n",
            "Name: ratings, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7uViOAP8nZ"
      },
      "source": [
        "### Train FFNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8cPuyIaPcsI",
        "outputId": "e7e60d44-7460-41f3-eaab-839abfa3e986"
      },
      "source": [
        "batch_size, epochs = 32, 5\n",
        "e = None\n",
        "model = NeuralNet(train_reviews, train_ratings, e)\n",
        "model.build_nn()\n",
        "model.train_nn(batch_size, epochs)\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "train_predictions = model.predict(train_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "train_predictions = np.argmax(train_predictions, axis=1)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "train_ratings = np.argmax(np.array(train_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(train_ratings, train_predictions)}\\n\")\n",
        "metrics.confusion_matrix(train_ratings, train_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews Shape:  (50000, 31)\n",
            "Ratings Shape:  (50000, 5)\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 73s 44ms/step - loss: 0.8538 - accuracy: 0.6985\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 67s 43ms/step - loss: 0.4484 - accuracy: 0.8426\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.1987 - accuracy: 0.9435\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.0914 - accuracy: 0.9760\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 68s 44ms/step - loss: 0.0492 - accuracy: 0.9881\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      4059\n",
            "           1       1.00      1.00      1.00      2265\n",
            "           2       1.00      0.99      1.00      3612\n",
            "           3       0.99      0.97      0.98      6871\n",
            "           4       0.99      1.00      1.00     33193\n",
            "\n",
            "    accuracy                           0.99     50000\n",
            "   macro avg       1.00      0.99      0.99     50000\n",
            "weighted avg       0.99      0.99      0.99     50000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4053,     0,     0,     3,     3],\n",
              "       [    2,  2256,     0,     2,     5],\n",
              "       [    1,     1,  3591,     7,    12],\n",
              "       [    0,     0,     2,  6681,   188],\n",
              "       [    0,     1,     4,    55, 33133]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2iF2egfUspg"
      },
      "source": [
        "#### Test accuracy model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foQ-CrQnQHyL",
        "outputId": "34ae1af8-090b-4910-c91e-09d887d22273"
      },
      "source": [
        "gold_test_data = pd.read_csv(\"gold_test.csv\", index_col=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "gold_test_reviews = gold_test_data.iloc[:, :-1]\n",
        "gold_test_ratings = gold_test_data.iloc[:, -1]\n",
        "gold_test_ratings = pd.get_dummies(gold_test_ratings)\n",
        "\n",
        "# pre-process data\n",
        "gold_test_reviews, tokenizer = preprocess_data(tokenizer, gold_test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(gold_test_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "test_ratings = np.argmax(np.array(gold_test_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(test_ratings, test_predictions)}\\n\")\n",
        "metrics.confusion_matrix(test_ratings, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.57      0.59      1271\n",
            "           1       0.22      0.15      0.18       630\n",
            "           2       0.32      0.27      0.29       911\n",
            "           3       0.29      0.25      0.27      1404\n",
            "           4       0.78      0.86      0.82      5784\n",
            "\n",
            "    accuracy                           0.64     10000\n",
            "   macro avg       0.44      0.42      0.43     10000\n",
            "weighted avg       0.61      0.64      0.62     10000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 728,  157,  123,   65,  198],\n",
              "       [ 218,   93,  114,   70,  135],\n",
              "       [ 115,   86,  246,  203,  261],\n",
              "       [  44,   34,  177,  348,  801],\n",
              "       [ 105,   44,  117,  518, 5000]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiB2cRSVQlZg"
      },
      "source": [
        "#### Test input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1gB_zAqQkrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24980992-0646-4dc3-ce84-ae7cf7dcedc1"
      },
      "source": [
        "# Test input\n",
        "Test = ['this is bad', 'wow nice!', 'this is a great product']\n",
        "test_reviews = pd.DataFrame(Test, columns=['reviews'])\n",
        "\n",
        "# pre-process data\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "\n",
        "# show probabilities\n",
        "print(test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.0704581e-01 1.2625413e-02 4.3810424e-03 9.9125065e-02 3.7682271e-01]\n",
            " [6.5364206e-05 5.0577797e-05 8.5448126e-05 1.5498983e-02 9.8429966e-01]\n",
            " [1.3302909e-03 4.3222381e-04 1.0145135e-03 2.0774772e-02 9.7644824e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyY0ZJ_U9XV5"
      },
      "source": [
        "#### Testing on custom input GUI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoBnu4pB9eF4"
      },
      "source": [
        "import tabulate\n",
        "def predict_rating(text):\n",
        "    test_reviews = pd.DataFrame([text], columns=['reviews'])\n",
        "\n",
        "    test_reviews, _ = preprocess_data(tokenizer, test_reviews, maxlen=maxlen)\n",
        "\n",
        "    test_predictions = model.predict(test_reviews)\n",
        "    test_ratings = np.argmax(np.array(test_predictions), axis=1) + 1\n",
        "    print(tabulate.tabulate(test_predictions, headers=['Rating-1', 'Rating-2', 'Rating-3', 'Rating-4', 'Rating-5']))\n",
        "    str = f\"\\nPredicted Rating: {test_ratings}\"\n",
        "    return str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7Eg9rqGQ9ckJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e18428a2-90cb-48ad-a363-92685fbd3552"
      },
      "source": [
        "#@title Predict\n",
        "InputText = 'this is good' #@param {type: 'string'}\n",
        "output = predict_rating(InputText)\n",
        "\n",
        "print(output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Rating-1    Rating-2    Rating-3    Rating-4    Rating-5\n",
            "----------  ----------  ----------  ----------  ----------\n",
            " 0.0306781  0.00405348  0.00437353   0.0557132    0.905182\n",
            "\n",
            "Predicted Rating: [5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmxT71tvjYyw"
      },
      "source": [
        "### Train LSTM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIRyqQEJkq53"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7uLPiFfsD6Q",
        "outputId": "157efd5b-3d63-4bea-c7b6-23d0bbce1661"
      },
      "source": [
        "batch_size, epochs = 32, 5\n",
        "\n",
        "model = LSTMNet(train_reviews, train_ratings, e)\n",
        "model.build_nn()\n",
        "model.train_nn(batch_size, epochs)\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "train_predictions = model.predict(train_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "train_predictions = np.argmax(train_predictions, axis=1)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "train_ratings = np.argmax(np.array(train_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(train_ratings, train_predictions)}\\n\")\n",
        "metrics.confusion_matrix(train_ratings, train_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews Shape:  (50000, 31)\n",
            "Ratings Shape:  (50000, 5)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 31, 300)           4786800   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "Softmax (Activation)         (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 5,007,093\n",
            "Trainable params: 5,007,093\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 75s 46ms/step - loss: 0.8982 - accuracy: 0.6954\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.6328 - accuracy: 0.7624\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 73s 47ms/step - loss: 0.5370 - accuracy: 0.7988\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.4440 - accuracy: 0.8350\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.3662 - accuracy: 0.8653\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89      4059\n",
            "           1       0.68      0.75      0.71      2265\n",
            "           2       0.85      0.67      0.75      3612\n",
            "           3       0.84      0.65      0.73      6871\n",
            "           4       0.93      0.99      0.96     33193\n",
            "\n",
            "    accuracy                           0.90     50000\n",
            "   macro avg       0.84      0.79      0.81     50000\n",
            "weighted avg       0.90      0.90      0.89     50000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3583,   344,    36,    12,    84],\n",
              "       [  267,  1691,   205,    35,    67],\n",
              "       [   62,   392,  2411,   500,   247],\n",
              "       [   16,    43,   153,  4476,  2183],\n",
              "       [   34,    18,    19,   324, 32798]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAhqvraOovkK"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNSqRS1youhm",
        "outputId": "ef7043c8-174b-4d21-cf36-81499331b8e5"
      },
      "source": [
        "gold_test_data = pd.read_csv(\"gold_test.csv\", index_col=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "gold_test_reviews = gold_test_data.iloc[:, :-1]\n",
        "gold_test_ratings = gold_test_data.iloc[:, -1]\n",
        "gold_test_ratings = pd.get_dummies(gold_test_ratings)\n",
        "\n",
        "# pre-process data\n",
        "gold_test_reviews, tokenizer = preprocess_data(tokenizer, gold_test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(gold_test_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "test_ratings = np.argmax(np.array(gold_test_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(test_ratings, test_predictions)}\\n\")\n",
        "metrics.confusion_matrix(test_ratings, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.60      0.63      1271\n",
            "           1       0.24      0.27      0.25       630\n",
            "           2       0.38      0.27      0.31       911\n",
            "           3       0.36      0.27      0.31      1404\n",
            "           4       0.81      0.90      0.86      5784\n",
            "\n",
            "    accuracy                           0.68     10000\n",
            "   macro avg       0.49      0.46      0.47     10000\n",
            "weighted avg       0.65      0.68      0.66     10000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 761,  274,   69,   47,  120],\n",
              "       [ 186,  167,  121,   50,  106],\n",
              "       [ 100,  170,  246,  200,  195],\n",
              "       [  37,   52,  141,  377,  797],\n",
              "       [  67,   43,   76,  364, 5234]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffNSrm9rt2kA"
      },
      "source": [
        "### Train RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dhveSHet2kD"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIJE5cUxt2kJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0048affe-122d-4b0e-9c77-6b15e5a56642"
      },
      "source": [
        "batch_size, epochs = 32, 5\n",
        "\n",
        "model = RNNNet(train_reviews, train_ratings, e)\n",
        "model.build_nn()\n",
        "model.train_nn(batch_size, epochs)\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "train_predictions = model.predict(train_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "train_predictions = np.argmax(train_predictions, axis=1)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "train_ratings = np.argmax(np.array(train_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(train_ratings, train_predictions)}\\n\")\n",
        "metrics.confusion_matrix(train_ratings, train_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews Shape:  (50000, 31)\n",
            "Ratings Shape:  (50000, 5)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 31, 300)           4786800   \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 128)               54912     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "Softmax (Activation)         (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 4,842,357\n",
            "Trainable params: 4,842,357\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 100s 63ms/step - loss: 1.0820 - accuracy: 0.6582\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 99s 63ms/step - loss: 0.9768 - accuracy: 0.6641\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 97s 62ms/step - loss: 0.9411 - accuracy: 0.6711\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 97s 62ms/step - loss: 0.8937 - accuracy: 0.6843\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 97s 62ms/step - loss: 0.8205 - accuracy: 0.6998\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.73      0.47      4059\n",
            "           1       0.15      0.01      0.01      2265\n",
            "           2       0.22      0.05      0.08      3612\n",
            "           3       0.29      0.01      0.02      6871\n",
            "           4       0.79      0.97      0.87     33193\n",
            "\n",
            "    accuracy                           0.71     50000\n",
            "   macro avg       0.36      0.35      0.29     50000\n",
            "weighted avg       0.62      0.71      0.63     50000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2964,    24,   141,    36,   894],\n",
              "       [ 1446,    13,    99,    35,   672],\n",
              "       [ 1940,    19,   165,    55,  1433],\n",
              "       [ 1257,    17,   148,    72,  5377],\n",
              "       [  887,    14,   182,    53, 32057]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zpgo68Ut2kN"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR3H2NpLt2kP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d695db1-05aa-4074-a440-d46334a81b21"
      },
      "source": [
        "gold_test_data = pd.read_csv(\"gold_test.csv\", index_col=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "gold_test_reviews = gold_test_data.iloc[:, :-1]\n",
        "gold_test_ratings = gold_test_data.iloc[:, -1]\n",
        "gold_test_ratings = pd.get_dummies(gold_test_ratings)\n",
        "\n",
        "# pre-process data\n",
        "gold_test_reviews, tokenizer = preprocess_data(tokenizer, gold_test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(gold_test_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "test_ratings = np.argmax(np.array(gold_test_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(test_ratings, test_predictions)}\\n\")\n",
        "metrics.confusion_matrix(test_ratings, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.70      0.49      1271\n",
            "           1       0.19      0.01      0.02       630\n",
            "           2       0.24      0.05      0.08       911\n",
            "           3       0.25      0.01      0.02      1404\n",
            "           4       0.74      0.94      0.83      5784\n",
            "\n",
            "    accuracy                           0.64     10000\n",
            "   macro avg       0.36      0.34      0.29     10000\n",
            "weighted avg       0.55      0.64      0.55     10000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 893,   10,   45,   14,  309],\n",
              "       [ 405,    5,   19,    5,  196],\n",
              "       [ 481,    8,   41,   12,  369],\n",
              "       [ 324,    3,   28,   16, 1033],\n",
              "       [ 291,    1,   36,   16, 5440]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwBUlb6e08Jc"
      },
      "source": [
        "### Train Bi-LSTM Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQfAyZJw02Wi"
      },
      "source": [
        "#### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDGrEbM-0rCl",
        "outputId": "894fb7b8-7e0b-4d2c-9122-3891d6548c4b"
      },
      "source": [
        "batch_size, epochs = 32, 5\n",
        "\n",
        "model = BiLSTMNet(train_reviews, train_ratings, e)\n",
        "model.build_nn()\n",
        "model.train_nn(batch_size, epochs)\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "train_predictions = model.predict(train_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "train_predictions = np.argmax(train_predictions, axis=1)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "train_ratings = np.argmax(np.array(train_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(train_ratings, train_predictions)}\\n\")\n",
        "metrics.confusion_matrix(train_ratings, train_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews Shape:  (50000, 31)\n",
            "Ratings Shape:  (50000, 5)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 31, 300)           4786800   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 1285      \n",
            "_________________________________________________________________\n",
            "Softmax (Activation)         (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 5,227,381\n",
            "Trainable params: 5,227,381\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 82s 49ms/step - loss: 0.8207 - accuracy: 0.7065\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.6017 - accuracy: 0.7702\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.4978 - accuracy: 0.8132\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.3854 - accuracy: 0.8585\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 0.2943 - accuracy: 0.8929\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.92      0.94      4059\n",
            "           1       0.87      0.79      0.83      2265\n",
            "           2       0.81      0.87      0.84      3612\n",
            "           3       0.86      0.78      0.82      6871\n",
            "           4       0.96      0.98      0.97     33193\n",
            "\n",
            "    accuracy                           0.93     50000\n",
            "   macro avg       0.89      0.87      0.88     50000\n",
            "weighted avg       0.93      0.93      0.93     50000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3747,   166,    92,    12,    42],\n",
              "       [  146,  1794,   270,    36,    19],\n",
              "       [   37,    91,  3140,   249,    95],\n",
              "       [    3,    14,   305,  5334,  1215],\n",
              "       [   19,     6,    74,   586, 32508]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJYLHRMe6JYZ"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m18uDQe6MYq",
        "outputId": "79803f04-e7b1-4f58-c48f-a3398ce9534d"
      },
      "source": [
        "gold_test_data = pd.read_csv(\"gold_test.csv\", index_col=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "gold_test_reviews = gold_test_data.iloc[:, :-1]\n",
        "gold_test_ratings = gold_test_data.iloc[:, -1]\n",
        "gold_test_ratings = pd.get_dummies(gold_test_ratings)\n",
        "\n",
        "# pre-process data\n",
        "gold_test_reviews, tokenizer = preprocess_data(tokenizer, gold_test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(gold_test_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "test_ratings = np.argmax(np.array(gold_test_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(test_ratings, test_predictions)}\\n\")\n",
        "metrics.confusion_matrix(test_ratings, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.58      0.62      1271\n",
            "           1       0.23      0.19      0.21       630\n",
            "           2       0.33      0.40      0.36       911\n",
            "           3       0.33      0.28      0.30      1404\n",
            "           4       0.83      0.86      0.84      5784\n",
            "\n",
            "    accuracy                           0.66     10000\n",
            "   macro avg       0.47      0.46      0.47     10000\n",
            "weighted avg       0.65      0.66      0.65     10000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 738,  224,  166,   36,  107],\n",
              "       [ 196,  122,  190,   50,   72],\n",
              "       [  91,  114,  363,  185,  158],\n",
              "       [  25,   34,  245,  395,  705],\n",
              "       [  70,   46,  152,  534, 4982]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yure0yC-Pcx5"
      },
      "source": [
        "### Train GRU Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrr7DoVDPiJf"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRn5KUHVPj0o",
        "outputId": "26c31db1-7633-4aba-96e9-21d0f0dd8cdc"
      },
      "source": [
        "batch_size, epochs = 32, 5\n",
        "\n",
        "model = GRUmodel(train_reviews, train_ratings, e)\n",
        "model.build_nn()\n",
        "model.train_nn(batch_size, epochs)\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "train_predictions = model.predict(train_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "train_predictions = np.argmax(train_predictions, axis=1)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "train_ratings = np.argmax(np.array(train_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(train_ratings, train_predictions)}\\n\")\n",
        "metrics.confusion_matrix(train_ratings, train_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews Shape:  (50000, 31)\n",
            "Ratings Shape:  (50000, 5)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 31, 300)           4786800   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 128)               165120    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "Softmax (Activation)         (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 4,952,565\n",
            "Trainable params: 4,952,565\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 108s 48ms/step - loss: 0.9218 - accuracy: 0.6909\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.6029 - accuracy: 0.7719\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 75s 48ms/step - loss: 0.4925 - accuracy: 0.8146\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.3764 - accuracy: 0.8624\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 76s 49ms/step - loss: 0.2812 - accuracy: 0.8993\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.95      0.93      4059\n",
            "           1       0.87      0.81      0.84      2265\n",
            "           2       0.87      0.83      0.85      3612\n",
            "           3       0.91      0.73      0.81      6871\n",
            "           4       0.95      0.99      0.97     33193\n",
            "\n",
            "    accuracy                           0.93     50000\n",
            "   macro avg       0.90      0.86      0.88     50000\n",
            "weighted avg       0.93      0.93      0.93     50000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3868,    82,    46,    13,    50],\n",
              "       [  259,  1832,   109,    24,    41],\n",
              "       [   86,   172,  2981,   191,   182],\n",
              "       [   27,    21,   232,  5021,  1570],\n",
              "       [   28,     3,    55,   245, 32862]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPLYwLDdPrHu"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7ExQM0jPs5v",
        "outputId": "5c41fccb-9468-4966-c134-99b91d83c34c"
      },
      "source": [
        "gold_test_data = pd.read_csv(\"gold_test.csv\", index_col=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "gold_test_reviews = gold_test_data.iloc[:, :-1]\n",
        "gold_test_ratings = gold_test_data.iloc[:, -1]\n",
        "gold_test_ratings = pd.get_dummies(gold_test_ratings)\n",
        "\n",
        "# pre-process data\n",
        "gold_test_reviews, tokenizer = preprocess_data(tokenizer, gold_test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(gold_test_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "test_ratings = np.argmax(np.array(gold_test_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(test_ratings, test_predictions)}\\n\")\n",
        "metrics.confusion_matrix(test_ratings, test_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.64      0.63      1271\n",
            "           1       0.24      0.21      0.23       630\n",
            "           2       0.35      0.29      0.32       911\n",
            "           3       0.32      0.22      0.26      1404\n",
            "           4       0.81      0.89      0.85      5784\n",
            "\n",
            "    accuracy                           0.67     10000\n",
            "   macro avg       0.47      0.45      0.46     10000\n",
            "weighted avg       0.64      0.67      0.65     10000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 814,  192,   98,   48,  119],\n",
              "       [ 216,  133,  123,   53,  105],\n",
              "       [ 129,  140,  268,  164,  210],\n",
              "       [  55,   35,  186,  312,  816],\n",
              "       [  94,   43,   85,  398, 5164]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5OCIWw_6VWI"
      },
      "source": [
        "### Train Bi-GRU Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e87HOz1C6ZrL"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkPL-ADb6Yin",
        "outputId": "add2af19-a581-419f-dc67-dfa477e45438"
      },
      "source": [
        "batch_size, epochs = 32, 5\n",
        "\n",
        "model = BiGRUmodel(train_reviews, train_ratings, e)\n",
        "model.build_nn()\n",
        "model.train_nn(batch_size, epochs)\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "train_predictions = model.predict(train_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "train_predictions = np.argmax(train_predictions, axis=1)\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "train_ratings = np.argmax(np.array(train_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(train_ratings, train_predictions)}\\n\")\n",
        "metrics.confusion_matrix(train_ratings, train_predictions)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reviews Shape:  (50000, 31)\n",
            "Ratings Shape:  (50000, 5)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 31, 300)           4786800   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               330240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 1285      \n",
            "_________________________________________________________________\n",
            "Softmax (Activation)         (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 5,118,325\n",
            "Trainable params: 331,525\n",
            "Non-trainable params: 4,786,800\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 45s 7ms/step - loss: 0.8510 - accuracy: 0.6974\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6837 - accuracy: 0.7450\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.6339 - accuracy: 0.7631\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5858 - accuracy: 0.7788\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.5387 - accuracy: 0.7933\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.87      0.79      4059\n",
            "           1       0.62      0.27      0.37      2265\n",
            "           2       0.60      0.56      0.58      3612\n",
            "           3       0.74      0.30      0.42      6871\n",
            "           4       0.86      0.99      0.92     33193\n",
            "\n",
            "    accuracy                           0.82     50000\n",
            "   macro avg       0.71      0.59      0.62     50000\n",
            "weighted avg       0.80      0.82      0.79     50000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3515,   150,   165,    17,   212],\n",
              "       [  921,   602,   495,    42,   205],\n",
              "       [  334,   180,  2007,   356,   735],\n",
              "       [   65,    29,   542,  2031,  4204],\n",
              "       [   52,     6,   141,   294, 32700]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrpft3716fnl"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8CPioHm6hSy",
        "outputId": "dc79f5aa-3285-4492-b54c-930ded9a63c9"
      },
      "source": [
        "gold_test_data = pd.read_csv(\"gold_test.csv\", index_col=0)\n",
        "\n",
        "# separate reviews, ratings\n",
        "gold_test_reviews = gold_test_data.iloc[:, :-1]\n",
        "gold_test_ratings = gold_test_data.iloc[:, -1]\n",
        "gold_test_ratings = pd.get_dummies(gold_test_ratings)\n",
        "\n",
        "# pre-process data\n",
        "gold_test_reviews, tokenizer = preprocess_data(tokenizer, gold_test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(gold_test_reviews)\n",
        "\n",
        "# get ratings from probabilities\n",
        "test_predictions = np.argmax(test_predictions, axis=1)\n",
        "test_ratings = np.argmax(np.array(gold_test_ratings), axis=1)\n",
        "\n",
        "# report generation on training data\n",
        "print(f\"Classification report:\\n{metrics.classification_report(test_ratings, test_predictions)}\\n\")\n",
        "metrics.confusion_matrix(test_ratings, test_predictions)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.74      0.69      1271\n",
            "           1       0.33      0.12      0.17       630\n",
            "           2       0.41      0.39      0.40       911\n",
            "           3       0.48      0.17      0.25      1404\n",
            "           4       0.79      0.96      0.87      5784\n",
            "\n",
            "    accuracy                           0.71     10000\n",
            "   macro avg       0.53      0.47      0.48     10000\n",
            "weighted avg       0.67      0.71      0.67     10000\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 940,   74,   97,   17,  143],\n",
              "       [ 270,   74,  155,   24,  107],\n",
              "       [ 154,   52,  351,  110,  244],\n",
              "       [  34,   11,  175,  242,  942],\n",
              "       [  54,   13,   69,  109, 5539]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FvG8LECtasu"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2USJAdpatdub",
        "outputId": "ddd04938-918e-488a-cda9-19c3f78cfa2b"
      },
      "source": [
        "model.model.save('model')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses, gru_cell_2_layer_call_fn, gru_cell_1_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyQbA9Rq2aSz"
      },
      "source": [
        "!tar -czf model.tar.gz model/"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y21WsTN3qK6",
        "outputId": "4ec2886d-d376-4969-fd55-24ecd2c0740d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk0pgB_REpb3"
      },
      "source": [
        "import pickle \n",
        "\n",
        "with open(r\"tokenizer.pkl\", \"wb\") as output_file:\n",
        "  pickle.dump(tokenizer, output_file)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be7go5zCFJZf",
        "outputId": "eb45c854-fe37-4fac-f842-0da8b796c1e6"
      },
      "source": [
        "with open(r\"tokenizer.pkl\", \"rb\") as input_file:\n",
        "  x = pickle.load(input_file)\n",
        "  \n",
        "print(x)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras_preprocessing.text.Tokenizer object at 0x7f5f7d9cd790>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMt0-JVhdVXN"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "4bWwhachdXN7",
        "outputId": "56145c35-c87b-47b0-b96b-197e8143fcb0"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# array = metrics.confusion_matrix(test_ratings, test_predictions)\n",
        "array = metrics.confusion_matrix(test_ratings, test_predictions)\n",
        "df_cm = pd.DataFrame(array, index = [i for i in range(1, 6)],\n",
        "                  columns = [i for i in range(1, 6)])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5f7c977350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95xAWH6bekc5"
      },
      "source": [
        "### Test input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRiR7zu-ekc_"
      },
      "source": [
        "# Test input\n",
        "Test = ['this is bad', 'wow nice!', 'this is a great product']\n",
        "test_reviews = pd.DataFrame(Test, columns=['reviews'])\n",
        "\n",
        "# pre-process data\n",
        "test_reviews, tokenizer = preprocess_data(tokenizer, test_reviews, maxlen=train_reviews.shape[1])\n",
        "\n",
        "# predict\n",
        "test_predictions = model.predict(test_reviews)\n",
        "\n",
        "# show probabilities\n",
        "print(test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOz_Tq6xekdE"
      },
      "source": [
        "### Testing on custom input GUI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRaGh3XfekdJ"
      },
      "source": [
        "import tabulate\n",
        "def predict_rating(text):\n",
        "    test_reviews = pd.DataFrame([text], columns=['reviews'])\n",
        "\n",
        "    test_reviews, _ = preprocess_data(tokenizer, test_reviews, maxlen=maxlen)\n",
        "\n",
        "    test_predictions = model.predict(test_reviews)\n",
        "    test_ratings = np.argmax(np.array(test_predictions), axis=1) + 1\n",
        "    print(tabulate.tabulate(np.round(test_predictions, 3), headers=['Rating-1', 'Rating-2', 'Rating-3', 'Rating-4', 'Rating-5']))\n",
        "    str = f\"\\nPredicted Rating: {test_ratings}\"\n",
        "    return str"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyBP9u4BekdM",
        "outputId": "dbfb4350-32d8-4ab1-a30d-8690fdbf3ad2"
      },
      "source": [
        "#@title Predict\n",
        "InputText = 'wow, the product is horrible' #@param {type: 'string'}\n",
        "output = predict_rating(InputText)\n",
        "\n",
        "print(output)\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Rating-1    Rating-2    Rating-3    Rating-4    Rating-5\n",
            "----------  ----------  ----------  ----------  ----------\n",
            "     0.883       0.048       0.024       0.006       0.039\n",
            "\n",
            "Predicted Rating: [1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}